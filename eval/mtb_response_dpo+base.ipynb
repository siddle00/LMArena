{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2f1d2-a2bf-4cdb-b88d-3581a57e0c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'writing', 'prompt': ['Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.', 'Rewrite your previous response. Start every sentence with the letter A.'], 'reference': [], 'prompt_id': 44067482}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521682fbd8ed409f9b92b570a1f264af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MT-Bench Response Generation using the FT-DPO model\n",
    "\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# Config\n",
    "BASE_MODEL       = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "DPO_ADAPTER_PATH = \"./dpo_arena55k_0830_dulcet_glade_12\"  \n",
    "OUT_DIR          = \"mtbench_runs\"\n",
    "MAX_NEW_TOKENS   = 1000\n",
    "TEMPERATURE      = 0.2\n",
    "TOP_P            = 0.9\n",
    "SEED             = 25\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Load MT-Bench prompts\n",
    "mtb = load_dataset(\"HuggingFaceH4/mt_bench_prompts\", split=\"train\")\n",
    "print(mtb[0])\n",
    "\n",
    "# Load models & tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "\n",
    "def load_base():\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        device_map=\"balanced\",\n",
    "    )\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dpo():\n",
    "    base = load_base()\n",
    "    dpo = PeftModel.from_pretrained(base, DPO_ADAPTER_PATH, device_map=\"auto\")\n",
    "    dpo.eval()\n",
    "    dpo.config.pad_token_id = tok.pad_token_id\n",
    "    return dpo\n",
    "\n",
    "\n",
    "dpo_model  = load_dpo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810f2eb-69f2-4f9f-945a-185bea0741e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation helper\n",
    "\n",
    "def chat_gen(model, turns):\n",
    "    \"\"\"\n",
    "    turns: list of user turns (1â€“2 strings)\n",
    "    returns: list of assistant replies\n",
    "    \"\"\"\n",
    "    replies = []\n",
    "    messages = [{\"role\": \"user\", \"content\": turns[0]}]\n",
    "\n",
    "    def run_once(msgs):\n",
    "        # 1) Build a single string with the chat template\n",
    "        prompt_text = tok.apply_chat_template(\n",
    "            msgs,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,            \n",
    "        )\n",
    "        # 2) Tokenize to get attention_mask\n",
    "        batch = tok(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=False,              \n",
    "            add_special_tokens=False,\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],  \n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                do_sample=bool(TEMPERATURE > 0),\n",
    "                pad_token_id=tok.pad_token_id,\n",
    "                eos_token_id=tok.eos_token_id,\n",
    "            )\n",
    "    \n",
    "        # decode only the generated tail\n",
    "        gen = tok.decode(out[0][batch[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        return gen.strip()\n",
    "\n",
    "    # turn 1\n",
    "    r1 = run_once(messages)\n",
    "    replies.append(r1)\n",
    "\n",
    "\n",
    "    # turn 2 if present\n",
    "    if len(turns) > 1 and turns[1]:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": r1})\n",
    "        messages.append({\"role\": \"user\", \"content\": turns[1]})\n",
    "        r2 = run_once(messages)\n",
    "        replies.append(r2)\n",
    "\n",
    "    return replies\n",
    "\n",
    "\n",
    "def run_model_on_mtbench(model, tag):\n",
    "    rows = []\n",
    "    for i, ex in enumerate(tqdm(mtb, desc=f\"Generating: {tag}\")):\n",
    "        turns = [t for t in ex[\"prompt\"] if isinstance(t, str) and t.strip()]\n",
    "        try:\n",
    "            replies = chat_gen(model, turns)\n",
    "        except Exception as e:\n",
    "            replies = [f\"[GENERATION ERROR] {e}\"]\n",
    "        rows.append({\n",
    "            \"index\": i,\n",
    "            \"category\": ex.get(\"category\"),\n",
    "            \"turns\": turns,\n",
    "            \"replies\": replies,\n",
    "            \"model_id\": tag,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Generate & Save\n",
    "df_dpo  = run_model_on_mtbench(dpo_model,  \"dpo\")\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "df_dpo.to_json(f\"{OUT_DIR}/dpo_generations.json\",  orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Saved generations to\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
