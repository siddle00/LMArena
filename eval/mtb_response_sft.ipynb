{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb5ef8b-1213-4b67-aba2-3f88eb764812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MT-Bench prompts: 80 items\n",
      "Example: {'category': 'writing', 'prompt': ['Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.', 'Rewrite your previous response. Start every sentence with the letter A.'], 'reference': [], 'prompt_id': 44067482}\n"
     ]
    }
   ],
   "source": [
    "# MT-Bench: SFT generations only\n",
    "# !pip install -q transformers accelerate datasets peft tqdm pandas\n",
    "\n",
    "import os, json, random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# Config\n",
    "BASE_MODEL        = os.environ.get(\"BASE_MODEL\", \"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "SFT_ADAPTER_PATH  = os.environ.get(\"SFT_ADAPTER_PATH\", \"./sft-mistral7b-arena55k\") \n",
    "\n",
    "\n",
    "# sampling\n",
    "MAX_NEW_TOKENS    = int(os.environ.get(\"MAX_NEW_TOKENS\", 1000))\n",
    "TEMPERATURE       = float(os.environ.get(\"TEMPERATURE\", 0.2))\n",
    "TOP_P             = float(os.environ.get(\"TOP_P\", 0.95))\n",
    "BATCH_SIZE        = int(os.environ.get(\"BATCH_SIZE\", 16))\n",
    "\n",
    "SEED              = int(os.environ.get(\"SEED\", 42))\n",
    "DEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_BF16          = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Load MT-Bench prompts\n",
    "# Each row: {'prompt': ['turn1', 'turn2?'], 'category': ...}\n",
    "\n",
    "mtb = load_dataset(\"HuggingFaceH4/mt_bench_prompts\", split=\"train\")\n",
    "print(f\"Loaded MT-Bench prompts: {len(mtb)} items\")\n",
    "print(\"Example:\", mtb[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eeb530-549c-42a5-8731-eca0f270e98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79a35781b434e258e8c272c1f5e2d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc932c2d0379483d825ea0490b4bf731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate R1: sft:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c522293c8e4f0e8690077513eab5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate R2: sft:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved SFT generations to mtbench_runs/sft_generations_1.json\n",
      "   index category                                              turns  \\\n",
      "0      0  writing  [Compose an engaging travel blog post about a ...   \n",
      "1      1  writing  [Draft a professional email seeking your super...   \n",
      "\n",
      "                                             replies model_id  \n",
      "0  [Title: Aloha from Hawaii: Unforgettable Cultu...      sft  \n",
      "1  [Subject: Request for Feedback on Quarterly Fi...      sft  \n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer & SFT model (with LoRA)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "\n",
    "def load_sft_model():\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.bfloat16 if USE_BF16 else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # apply adapter\n",
    "    model = PeftModel.from_pretrained(base, SFT_ADAPTER_PATH, device_map=\"auto\")\n",
    "    model.eval()\n",
    "    model.config.use_cache = True\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "    model.config.eos_token_id = tok.eos_token_id\n",
    "    # optional: flash-attn if available\n",
    "    try:\n",
    "        model.config.attn_implementation = \"flash_attention_2\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return model\n",
    "\n",
    "\n",
    "sft_model = load_sft_model()\n",
    "\n",
    "# Batched generation utils (with attention_mask)\n",
    "def _apply_template(messages_list, tok):\n",
    "    # messages_list: list of chat messages lists\n",
    "    # returns list[str] of templated prompts (no tensors yet)\n",
    "    return [\n",
    "        tok.apply_chat_template(msgs, add_generation_prompt=True, tokenize=False)\n",
    "        for msgs in messages_list\n",
    "    ]\n",
    "\n",
    "\n",
    "def _encode_chat_texts(texts, tok, device):\n",
    "    # batch tokenize to get input_ids + attention_mask\n",
    "    batch = tok(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,              # enables batching\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "\n",
    "def _gen_batch(model, inputs, tok):\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "            do_sample=bool(TEMPERATURE > 0),\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    # decode only generated tails\n",
    "    gens = []\n",
    "    in_lens = inputs[\"input_ids\"].shape[1]\n",
    "    for i in range(out.size(0)):\n",
    "        text = tok.decode(out[i, in_lens:], skip_special_tokens=True).strip()\n",
    "        gens.append(text)\n",
    "    return gens\n",
    "\n",
    "\n",
    "def run_model_on_mtbench_batched(model, tag, mtb, tok, device, batch_size=BATCH_SIZE):\n",
    "    # 1) collect turns\n",
    "    recs = []\n",
    "    for i, ex in enumerate(mtb):\n",
    "        turns = [t for t in ex[\"prompt\"] if isinstance(t, str) and t.strip()]\n",
    "        recs.append({\"i\": i, \"category\": ex.get(\"category\"), \"turns\": turns})\n",
    "\n",
    "\n",
    "    # 2) round 1: first turns\n",
    "    msgs_r1 = [[{\"role\": \"user\", \"content\": r[\"turns\"][0]}] for r in recs]\n",
    "    texts_r1 = _apply_template(msgs_r1, tok)\n",
    "\n",
    "\n",
    "    replies1 = [None] * len(recs)\n",
    "    for start in tqdm(range(0, len(texts_r1), batch_size), desc=f\"Generate R1: {tag}\"):\n",
    "        batch_texts = texts_r1[start:start+batch_size]\n",
    "        inputs = _encode_chat_texts(batch_texts, tok, device)\n",
    "        gens = _gen_batch(model, inputs, tok)\n",
    "        replies1[start:start+batch_size] = gens\n",
    "\n",
    "\n",
    "    # 3) round 2: where needed\n",
    "    idx_second = [idx for idx, r in enumerate(recs) if len(r[\"turns\"]) > 1 and r[\"turns\"][1]]\n",
    "    msgs_r2, idx_map = [], []\n",
    "    for idx in idx_second:\n",
    "        msgs_r2.append([\n",
    "            {\"role\": \"user\", \"content\": recs[idx][\"turns\"][0]},\n",
    "            {\"role\": \"assistant\", \"content\": replies1[idx]},\n",
    "            {\"role\": \"user\", \"content\": recs[idx][\"turns\"][1]},\n",
    "        ])\n",
    "        idx_map.append(idx)\n",
    "\n",
    "\n",
    "    replies2 = [None] * len(recs)\n",
    "    if msgs_r2:\n",
    "        texts_r2 = _apply_template(msgs_r2, tok)\n",
    "        for start in tqdm(range(0, len(texts_r2), batch_size), desc=f\"Generate R2: {tag}\"):\n",
    "            batch_texts = texts_r2[start:start+batch_size]\n",
    "            inputs = _encode_chat_texts(batch_texts, tok, device)\n",
    "            gens = _gen_batch(model, inputs, tok)\n",
    "            for j, g in enumerate(gens):\n",
    "                replies2[idx_map[start + j]] = g\n",
    "\n",
    "\n",
    "    # 4) pack dataframe\n",
    "    rows = []\n",
    "    for idx, r in enumerate(recs):\n",
    "        rep = [replies1[idx]]\n",
    "        if replies2[idx] is not None:\n",
    "            rep.append(replies2[idx])\n",
    "        rows.append({\n",
    "            \"index\": r[\"i\"],\n",
    "            \"category\": r[\"category\"],\n",
    "            \"turns\": r[\"turns\"],\n",
    "            \"replies\": rep,\n",
    "            \"model_id\": tag,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Generate & Save (SFT only)\n",
    "df_sft = run_model_on_mtbench_batched(sft_model, \"sft\", mtb, tok, DEVICE, BATCH_SIZE)\n",
    "\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "out_path = os.path.join(OUT_DIR, \"sft_generations_1.json\")\n",
    "df_sft.to_json(out_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "\n",
    "print(\"Saved SFT generations to\", out_path)\n",
    "print(df_sft.head(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15264b2c-4f5f-4146-a9dc-b7620b0e5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sft.to_parquet('mt_bench_generation_sft_1.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
